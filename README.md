## Popular Articles

<p>
  <img height="128" align='left' src="https://teddykoker.com/images/signal_chain.png">
</p>

### [Deep Learning for Guitar Effect Emulation](https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/)

Weâ€™ll follow a deep learning approach to see if we can use machine learning to replicate the sound 
of an iconic analog effect pedal, the Ibanez Tube Screamer.
 
---

<p>
  <img height="128" align='left' src="https://teddykoker.com/images/betting.png">
</p>

### [Beating the Odds: Machine Learning for Horse Racing](https://teddykoker.com/2019/12/beating-the-odds-machine-learning-for-horse-racing/)

Inspired by the story of Bill Benter, a gambler who developed a computer model that made him close to a billion dollars1 betting on horse races in the Hong Kong Jockey Club (HKJC), I set out to see if I could use machine learning to identify inefficiencies in horse racing wagering.

---

<p>
  <img height="128" align='left' style="margin-right:14px" src="https://teddykoker.com/images/poly_circles.png">
</p>

### [Performers: The Kernel Trick, Random Fourier Features, and Attention](https://teddykoker.com/2020/11/performers/)

Google AI recently released a paper, *Rethinking Attention with Performers* (Choromanski et al., 2020), which introduces Performer, a Transformer architecture which estimates the full-rank-attention mechanism using orthogonal random features to approximate the softmax kernel with linear space and time complexity. In this post we will investigate how this works, and how it is useful for the machine learning community.
