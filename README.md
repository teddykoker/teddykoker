# Hi there ðŸ‘‹

## Popular Articles

<p>
  <img height="128" align='left' src="https://teddykoker.com/images/signal_chain.png">
</p>

### [Deep Learning for Guitar Effect Emulation](https://teddykoker.com/2020/05/deep-learning-for-guitar-effect-emulation/)

Weâ€™ll follow a deep learning approach to see if we can use machine learning to replicate the sound 
of an iconic analog effect pedal, the Ibanez Tube Screamer.
 
---

<p>
  <img height="128" align='left' src="https://teddykoker.com/images/betting.png">
</p>

### [Beating the Odds: Machine Learning for Horse Racing](https://teddykoker.com/2019/12/beating-the-odds-machine-learning-for-horse-racing/)

Inspired by the story of Bill Benter, a gambler who developed a computer model that made him close to a billion dollars1 betting on horse races in the Hong Kong Jockey Club (HKJC), I set out to see if I could use machine learning to identify inefficiencies in horse racing wagering.

---

<p>
  <img height="128" align='left' style="margin-right:14px" src="https://teddykoker.com/images/2020-02-25-nlp-from-scratch-annotated-attention_34_0.png">
</p>

### [NLP from Scratch: Annotated Attention](https://teddykoker.com/2020/02/nlp-from-scratch-annotated-attention/)

This post is the first in a series of articles about natural language processing (NLP), a subfield of machine learning concerning the interaction between computers and human language. This article will be focused on attention, a mechanism that forms the backbone of many state-of-the art language models, including Googleâ€™s BERT (Devlin et al., 2018), and OpenAIâ€™s GPT-2 (Radford et al., 2019).
